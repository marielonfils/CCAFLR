{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"570c24f2-126a-4fd3-bae2-e8247646f36d","_uuid":"118c9d31c593676aaf77c8975339e52d7cb65cdf"},"source":["inspired from https://www.kaggle.com/code/paultimothymooney/predict-idc-in-breast-cancer-histology-images and https://www.kaggle.com/code/zfturbo/mnist-with-mobilenet-pytorch-gpu"]},{"cell_type":"markdown","metadata":{"_cell_guid":"eb849bd9-7894-4fd3-b55d-4d0a77c9161c","_uuid":"debe60106fe79d3b2316308fd11f3d2399255c61"},"source":["*Step 1: Import Modules*"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"bb9fe3ef-104a-4a98-9f87-b65906acf50f","_kg_hide-input":true,"_uuid":"5a9827f8d703a8741175d1e7df3121632715bdd8","trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from glob import glob\n","import itertools\n","import fnmatch\n","import random\n","import matplotlib.pylab as plt\n","import seaborn as sns\n","import cv2\n","#from scipy.misc import imresize, imread\n","import sklearn\n","from sklearn import model_selection\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\n","from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","#import keras\n","#from keras import backend as K\n","#from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","#from keras.preprocessing.image import ImageDataGenerator\n","#from keras.utils.np_utils import to_categorical\n","#from keras.models import Sequential, model_from_json\n","#from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n","#from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"_cell_guid":"4c6bd2df-2a43-4604-b407-ba81c8c3939b","_uuid":"540f3c39324580297bcff94b4c4cfc0c5bb45b5f"},"source":["*Step 2: Explore Data*"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"765908c1-86cc-41df-8479-a39174231237","_uuid":"1978266e1e2fe5d6eb9bdc17deb0d309441065a2","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["./archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x3101_y1001_class1.png\n","./archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x2901_y1251_class1.png\n","./archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x1401_y1001_class1.png\n","./archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x2801_y1151_class1.png\n","./archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x2901_y301_class1.png\n","./archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x2951_y1201_class1.png\n","./archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x2701_y901_class1.png\n","./archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x2951_y1301_class1.png\n","./archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x2451_y651_class1.png\n","./archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x1501_y851_class1.png\n"]}],"source":["imagePatches = glob('./archive/IDC_regular_ps50_idx5/**/*.png', recursive=True)\n","for filename in imagePatches[0:10]:\n","    print(filename)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"5f695df0-da48-4cc5-93a0-dcb7a82d21a3","_uuid":"c88fc872e6af3918192df09635e88904145761a2"},"source":["*Step 3: Preprocess Data*"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"939bc134-6122-4d4d-aa6c-89a47318896d","_uuid":"aae6a2ec2ab20bd94422e29b147a45a880fb480a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["IDC(-)\n","\n"," ['./archive/IDC_regular_ps50_idx5/9255/0/9255_idx5_x2551_y1001_class0.png', './archive/IDC_regular_ps50_idx5/9255/0/9255_idx5_x2301_y851_class0.png', './archive/IDC_regular_ps50_idx5/9255/0/9255_idx5_x1951_y1401_class0.png', './archive/IDC_regular_ps50_idx5/9255/0/9255_idx5_x2151_y1451_class0.png', './archive/IDC_regular_ps50_idx5/9255/0/9255_idx5_x1851_y801_class0.png'] \n","\n","IDC(+)\n","\n"," ['./archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x3101_y1001_class1.png', './archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x2901_y1251_class1.png', './archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x1401_y1001_class1.png', './archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x2801_y1151_class1.png', './archive/IDC_regular_ps50_idx5/9255/1/9255_idx5_x2901_y301_class1.png']\n"]}],"source":["patternZero = '*class0.png'\n","patternOne = '*class1.png'\n","classZero = fnmatch.filter(imagePatches, patternZero)\n","classOne = fnmatch.filter(imagePatches, patternOne)\n","print(\"IDC(-)\\n\\n\",classZero[0:5],'\\n')\n","print(\"IDC(+)\\n\\n\",classOne[0:5])"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"f62e9e56-6564-4376-845f-284f624fa932","_uuid":"69640054f59133ebe8dfa06612f3c14383916e90","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["def proc_images(lowerIndex,upperIndex):\n","    \"\"\"\n","    Returns two arrays: \n","        x is an array of resized images\n","        y is an array of labels\n","    \"\"\" \n","    x = []\n","    y = []\n","    WIDTH = 50\n","    HEIGHT = 50\n","    for img in imagePatches[lowerIndex:upperIndex]:\n","        full_size_image = cv2.imread(img)\n","        x.append(cv2.resize(full_size_image, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n","        if img in classZero:\n","            y.append(0)\n","        elif img in classOne:\n","            y.append(1)\n","        else:\n","            return\n","    return x,y"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"3bf67ef8-5613-4862-bc46-0106d1880981","_uuid":"2b8fd5414423f1e7ff7da620d8672bb496a9ef57","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["X,Y = proc_images(0,9000)\n","df = pd.DataFrame()\n","df[\"images\"]=X\n","df[\"labels\"]=Y\n","X2=df[\"images\"]\n","Y2=df[\"labels\"]\n","X2=np.array(X2)\n","imgs0=[]\n","imgs1=[]\n","imgs0 = X2[Y2==0] # (0 = no IDC, 1 = IDC)\n","imgs1 = X2[Y2==1] "]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"b3c1681f-7576-4b3d-ad49-7832e3bcf209","_uuid":"5660e633007abaeda0a4698b8bfacf55d9552ab4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of images: 9000\n","Number of IDC(-) Images: 5763\n","Number of IDC(+) Images: 3237\n","Percentage of positive images: 35.97%\n","Image shape (Width, Height, Channels): (50, 50, 3)\n"]}],"source":["def describeData(a,b):\n","    print('Total number of images: {}'.format(len(a)))\n","    print('Number of IDC(-) Images: {}'.format(np.sum(b==0)))\n","    print('Number of IDC(+) Images: {}'.format(np.sum(b==1)))\n","    print('Percentage of positive images: {:.2f}%'.format(100*np.mean(b)))\n","    print('Image shape (Width, Height, Channels): {}'.format(a[0].shape))\n","describeData(X2,Y2)"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"0c3b967b-674c-4efa-b59e-56772166ef41","_uuid":"9eed051f46211121a7add96c9cf86165340a7e28","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                                              images  labels\n","0  [[[173, 135, 195], [166, 122, 175], [145, 94, ...       1\n","1  [[[163, 117, 174], [152, 107, 161], [176, 137,...       1\n","2  [[[141, 96, 184], [159, 112, 184], [149, 107, ...       1\n","3  [[[124, 71, 132], [139, 95, 162], [133, 84, 15...       1\n","4  [[[132, 77, 127], [151, 101, 160], [140, 94, 1...       1\n","5  [[[160, 122, 189], [164, 125, 172], [170, 131,...       1\n","6  [[[113, 61, 111], [110, 57, 101], [90, 38, 81]...       1\n","7  [[[130, 83, 149], [157, 114, 179], [154, 113, ...       1\n","8  [[[132, 96, 134], [217, 200, 224], [170, 135, ...       1\n","9  [[[146, 101, 155], [145, 101, 150], [184, 149,...       1\n","\n","{0: 'IDC(-)', 1: 'IDC(+)'}\n"]}],"source":["dict_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n","print(df.head(10))\n","print(\"\")\n","print(dict_characters)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"254e9903-c303-46cd-bde9-0d3b412e6cdb","_uuid":"4d584d18cb934f1cf025e9bbb6d6cb8c03542494"},"source":["The data is scaled from 0 to 256 but we want it to be scaled from 0 to 1. This will make the data compatible with a wide variety of different classification algorithms.  We also want to set aside 20% of the data for testing. This will make the trained model less prone to overfitting.  And finally, we will use an oversampling strategy to deal with the imbalanced class sizes."]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"9f9e4c80-a8e0-47ac-957b-22231b1e9fca","_uuid":"ae82d3f7ec59d7d112a8dd9ed1b8121a4792a1b0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(9000, 50, 50, 3) (7200, 50, 50, 3)\n","Training Data Shape: (7200, 50, 50, 3)\n","Testing Data Shape: (1800, 50, 50, 3)\n"]}],"source":["X=np.array(X)\n","X2=X/255.0\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X2, Y, test_size=0.2)\n","print(X.shape, X_train.shape)\n","# Reduce Sample Size for DeBugging\n","X_train2 = X_train[0:300000] \n","Y_train2 = Y_train[0:300000]\n","X_test2 = X_test[0:300000] \n","Y_test2 = Y_test[0:300000]\n","\n","print(\"Training Data Shape:\", X_train2.shape)\n","print(\"Testing Data Shape:\", X_test2.shape)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["X_trainShape = X_train2.shape[1]*X_train2.shape[2]*X_train2.shape[3]\n","X_testShape = X_test2.shape[1]*X_test2.shape[2]*X_test2.shape[3]\n","X_trainFlat = X_train2.reshape(X_train2.shape[0], X_trainShape)\n","X_testFlat = X_test2.reshape(X_test2.shape[0], X_testShape)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["for i in range(len(X_trainFlat)):\n","    height, width, channels = 50,50,3\n","    X_trainFlat2 = X_trainFlat.reshape(len(X_trainFlat),channels,height,width)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["for i in range(len(X_testFlat)):\n","    height, width, channels = 50,50,3\n","    X_testFlat2 = X_testFlat.reshape(len(X_testFlat),channels,height,width)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(7200, 3, 50, 50)\n"]}],"source":["print(X_trainFlat2.shape)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class BreastDataset(Dataset):\n","    def __init__(self,data,labels,transform=None, target_transform=None) -> None:\n","        super().__init__()\n","        self.data=data\n","        self.labels=labels\n","        self.transform = transform\n","        self.target_transform = target_transform\n","    def __len__(self):\n","        return len(self.labels)\n","    def __getitem__(self, index):\n","        image,label= self.data[index],self.labels[index]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image,label"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["train_dataset = BreastDataset(X_trainFlat2,Y_train2)\n","test_dataset = BreastDataset(X_testFlat2,Y_test2)\n","train_loader = DataLoader(train_dataset, batch_size=32)#, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)#, shuffle=True)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from torchvision.models.mobilenet import mobilenet_v2\n","import torch\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","from torch.nn import CrossEntropyLoss"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["import torch.nn as nn"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def train(model, device, train_loader,labels, optimizer, epoch):\n","    log_interval = 10\n","    loss_func = CrossEntropyLoss()\n","    model.train()\n","    for batch_idx, (data,target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        output=model(data)\n","        loss = loss_func(output, target)\n","        loss.backward()\n","        model.optimizer.step()\n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def tst(model, device, test_loader,labels):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    loss_func = CrossEntropyLoss()\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            output = model(data)\n","            test_loss += loss_func(output, target)\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["class MobileNet(nn.Module):\n","    def __init__(self, optimizer, scheduler,num_classes=2):\n","        super(MobileNet, self).__init__()\n","        self.layers = mobilenet_v2(pretrained=True,)\n","        self.layers.classifier[1] = torch.nn.Linear(in_features=self.layers.classifier[1].in_features, out_features=10)\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","    def forward(self, x):\n","        return self.layers(x)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 3, 50, 50])\n"]}],"source":["for i in train_loader:\n","    a,b=i\n","    print(a.shape)\n","    break"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cpu Epochs: 1 Batch size: 1000\n"]},{"name":"stderr","output_type":"stream","text":["/home/mlonfils/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/home/mlonfils/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [0/7200 (0%)]\tLoss: 2.510541\n","Train Epoch: 1 [320/7200 (4%)]\tLoss: 1.059141\n","Train Epoch: 1 [640/7200 (9%)]\tLoss: 0.428643\n","Train Epoch: 1 [960/7200 (13%)]\tLoss: 0.496561\n","Train Epoch: 1 [1280/7200 (18%)]\tLoss: 0.351405\n","Train Epoch: 1 [1600/7200 (22%)]\tLoss: 0.589299\n","Train Epoch: 1 [1920/7200 (27%)]\tLoss: 0.387597\n","Train Epoch: 1 [2240/7200 (31%)]\tLoss: 0.224318\n","Train Epoch: 1 [2560/7200 (36%)]\tLoss: 0.772651\n","Train Epoch: 1 [2880/7200 (40%)]\tLoss: 0.294652\n","Train Epoch: 1 [3200/7200 (44%)]\tLoss: 0.405808\n","Train Epoch: 1 [3520/7200 (49%)]\tLoss: 0.412225\n","Train Epoch: 1 [3840/7200 (53%)]\tLoss: 0.442291\n","Train Epoch: 1 [4160/7200 (58%)]\tLoss: 0.448827\n","Train Epoch: 1 [4480/7200 (62%)]\tLoss: 0.393972\n","Train Epoch: 1 [4800/7200 (67%)]\tLoss: 0.364117\n","Train Epoch: 1 [5120/7200 (71%)]\tLoss: 0.323820\n","Train Epoch: 1 [5440/7200 (76%)]\tLoss: 0.450848\n","Train Epoch: 1 [5760/7200 (80%)]\tLoss: 0.388441\n","Train Epoch: 1 [6080/7200 (84%)]\tLoss: 0.306171\n","Train Epoch: 1 [6400/7200 (89%)]\tLoss: 0.219964\n","Train Epoch: 1 [6720/7200 (93%)]\tLoss: 0.371500\n","Train Epoch: 1 [7040/7200 (98%)]\tLoss: 0.345284\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     19\u001b[0m     train(model, device, train_loader,Y_train2, optimizer, epoch)\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mtst\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_test2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n","Cell \u001b[0;32mIn[25], line 8\u001b[0m, in \u001b[0;36mtst\u001b[0;34m(model, device, test_loader, labels)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m----> 8\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_func(output, target)\n\u001b[1;32m     10\u001b[0m         pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# get the index of the max log-probability\u001b[39;00m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36mMobileNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py:174\u001b[0m, in \u001b[0;36mMobileNetV2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py:166\u001b[0m, in \u001b[0;36mMobileNetV2._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Cannot use \"squeeze\" as batch-size can be 1\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(x, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py:64\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/modules/activation.py:234\u001b[0m, in \u001b[0;36mHardtanh.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhardtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Doctorat/fork/flower2/lib/python3.10/site-packages/torch/nn/functional.py:1522\u001b[0m, in \u001b[0;36mhardtanh\u001b[0;34m(input, min_val, max_val, inplace)\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(hardtanh, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, min_val\u001b[38;5;241m=\u001b[39mmin_val, max_val\u001b[38;5;241m=\u001b[39mmax_val, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m-> 1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhardtanh_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1524\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mhardtanh(\u001b[38;5;28minput\u001b[39m, min_val, max_val)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["batch_size = 1000\n","learning_rate = 1.0\n","reduce_lr_gamma = 0.7\n","epochs = 1\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('Device: {} Epochs: {} Batch size: {}'.format(device, epochs, batch_size))\n","\n","kwargs = {'batch_size': batch_size}\n","if torch.cuda.is_available():\n","    kwargs.update({'num_workers': 1, 'pin_memory': True})\n","optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n","\n","scheduler = StepLR(optimizer, step_size=1, gamma=reduce_lr_gamma)\n","\n","model = MobileNet(scheduler)\n","model.double()\n","model.to(device)\n","for epoch in range(1, epochs + 1):\n","    train(model, device, train_loader,Y_train2, optimizer, epoch)\n","    tst(model, device, test_loader,Y_test2)\n","    scheduler.step()\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":7415,"sourceId":10564,"sourceType":"datasetVersion"}],"dockerImageVersionId":46,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"flower2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
